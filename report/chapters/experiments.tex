\chapter{Experiments}
\label{chp:experiments}

As a platform, we used the DAS-4 and its MPI implementation. We run algorithm with 8 instances per physical node, which is default for the DAS-4.
The DAS scheduler is responsible for placing algorithms on nodes. To circumvent jitter in timings results, we repeat each experiment 5 times. We vary the number of worker nodes in our experiment. Due to limitations of the platform, we can either run a multiple of 8 workers or less than eight workers. This is because DAS places 8 workers on a single physical platform.

To test the previously described implementations of the Push Lift algorithm, we run it repeatedly in different configurations on several different data sets. The algorithm is run with 1, 2, 4, 8, 16, 24, 32, \textellipsis, 120 (powers of two up to eight, then multiples of 8 up to 120)\footnote{Even though the DAS cluster has enough nodes available for 128 workers, there was an outage preventing us from using the entire cluster. Thus, one node remained unused.} workers to see whether there is any speed up when running the algorithm in parallel.

\section{Experiment data}
For our experiment data, we used real-world graphs as well as generated graphs. For each graph we chose a specific source and sink for our experiments so that we could easily repeat and average results. An overview of the used graphs and configuration used can be found in \autoref{tab:data-overview}. Not all configurations are shown, however.

The real-world sample graphs were obtained from the University of Florida Sparse Matrix Collection~\cite{FloridaSparseMatrix}. Although the collection is mostly unrelated to graphs, some matrices are in fact regular (natural) networks. We looked for well-connected graphs weighted graphs of a size for which max-flow computations take a significant but not too large time to complete. From the collection, we chose two graphs: \texttt{vanHeukelem/Cage11}, a biological network, and \texttt{Pajek/Internet}, a network topology of internet routers.

To generate graphs, we looked to the GTgraph~\cite{bader2006gtgraph} toolkit. This program can generate graphs using any of four algorithms:
\begin{description}
    \item[random] allows us to specify a number of nodes $n$ and a number of edges $m$, after which it iteratively picks two random nodes and adds an edge between them.

    \item[Erd\H{o}s-Renyi] takes a number of edges $n$ and a probability $p$ of an edge between any pair of nodes existing. Then it iterates over all pairs of nodes and generates an edge with probability $p$. This is mostly equivalent to the \textbf{random} generator when $p = \frac{m}{n \times (n - 1)}$.

    \item[R-MAT] generates graphs that follow a power law for the degree distribution of the nodes. This means that for any degree $k$, $P(k) \propto k^{-\gamma}$.\footnote{In directed graphs nodes have two degrees, $k_{in}$ and $k_{out}$ because the number of edges coming in is not necessarily the same as the number of edges going out. The power law distribution then simply applies to both separately.} This is a property commonly encountered in various networks~\cite{newman2005power}.

    \item[SSCA2] generates locally clustered graphs, meaning it creates graphs that are very well connected in small communities with little connections between them.
\end{description}

Because the both the random and Erd\H{o}s-Renyi algorithms do not produce graphs similar to those observed in real scenarios, we only consider graphs generated by the R-MAT and SSCA2 algorithms.

For each graph, we searched for a combination of source/sink that took a reasonable time to compute with one worker instance, in order to repeat the experiment and also had a non-zero final result so we could actually check the result. The resulting configurations can be found in \autoref{tab:data-overview}.

\begin{table}
	\centering

	\begin{tabularx}{\textwidth}{l | X | l | l | l | l | l}
		Name & Description & From & Nodes & Edges & Source & Sink \\
		\hline
		cage11 & DNA electrophoresis & \texttt{vanHeukelum/cage11} & 39082 & 559722 & 1361 & 28129 \\
		internet & Connectivity of internet routers & \texttt{Pajek/internet} & 124651 & 207214 & 94268 & 1046 \\
		rmat & Scale-free random graph & GTgraph R-MAT & 30000 & 5000000 & 89872 & 59366 \\
		ssca2 & Hierarchically clustered random graph & GTgraph SSCA2 & 32768 & 1570139 & 21264 & 7066 \\
	\end{tabularx}
	\caption{An overview of the data sets and configurations used.}
	\label{tab:data-overview}
\end{table}


\section{Expectations}
Even with the optimizations described in the previous chapter, the algorithm needs a lot of communication between workers about its current state. Therefore we will only see any real performance gains when the workers have to rarely wait for input and are mostly busy. This is not an ideal scenario.

Furthermore, since we run the algorithm with 8 workers on a single physical DAS node, the communication latency increases greatly after scaling past 8 workers. We expect to see this gap in the performance as well.

\section{Results}
After running every implementation of a varying amount of MPI workers, we can compare the execution time of each algorithm. For the results, we have looked at algorithm execution time only. An overview of that can be seen in \autoref{tab:algorithm-runtime}. It should be noted that the standard deviation on the runtime is fairly low compared to the actual runtime. This shows that despite network jitter and other external factors, the run time of the algorithm is rather consistent

Missing entries in the table (and the following graphs) are configurations that failed to complete within the time limit of two hours.

\begin{table}
    \centering
    \input{generated/runtime-table.tex}
    \caption{Mean and standard deviation for algorithm run times in several configurations.}
    \label{tab:algorithm-runtime}
\end{table}

We have decided to leave out the algorithm initialization time (i.e.\ initializing MPI and reading in the data) as this is unrelated to the problem being solved, and it would only serve to hide potential gains. Furthermore, as shown in \autoref{fig:initialization_time}, the cost of initializing does not increase that much with the number of workers, so even at larger scale this should not be a problem.

\begin{figure}
	\includegraphics[width=\textwidth]{graphs/graph_initialization}
  \caption{Cost of initialization as a function of the number of workers. This graph shows an average over all trial runs in this thesis, not one specific graph.}
  \label{fig:initialization_time}
\end{figure}

\subsection{Relative speed up}
What we were most interested in, was the possibility of a speed up when running the algorithm in parallel. In \autoref{fig:speedup_cage11}, we see the relative speed up of both working implementations on the various graphs. In this figure, we define the relative speed up as the time it took the algorithm to complete (on average) relative to the time it took that that implementation \emph{with the least number of workers to complete within two hours}. In other words, in the case of \texttt{cage11-impl2}, which first succeeded with 8 workers, the speed up is defined relative to that rather than to the result with 1 workers.

\begin{figure}
  \includegraphics[width=\textwidth]{graphs/graph_speedup}
  \caption{Relative speed up when running a specific route on the datasets.}
  \label{fig:speedup_cage11}
\end{figure}

The worker-axis is scaled logarithmically to highlight the performance gains within the first physical node, before external communication needs to occur. Most trends indeed seem to flatten at 8 workers, which is to be expected since the communication latency increases at that point.

As expected, the figure shows very little speed up across the board. In particular, implementation 1 hardly shows any performance gains at all while implementation 2 improves 3-5 fold while using 120 cores. This most likely is due to the massive overhead in communication.\footnote{In fact, the results produced by Hong et al \cite{LockFreeMultithreadedMaxFlow} on a shared memory implementation prove this, as shared memory has a lower overhead than a message passing implementation such as this one.}

\subsection{Comparing implementations}
Even though \autoref{fig:speedup_cage11} suggests that implementation 2 scales better when run in parallel than implementation 1, this is hardly fair. As seen in \autoref{tab:algorithm-runtime}, the run time of the former is always longer than that of the latter.

In \autoref{fig:implmementations} we show the run times of the different implementations on the same graph, and we can immediately see that, even without any speed up, the first implementation still is faster. The second merely gets closer in performance. This tells us that, for the second implementation, there is significantly more time being spent on computation and not communication. Only when the majority of time is being spent in communication (the networking systems were the same for both) rather than the actual algorithm, the run times start to get similar.

\begin{figure}[b!]
	\centering
  \includegraphics[width=0.9\textwidth]{graphs/graph_implementations}
  \caption{Comparison of implementations by execution time on the \texttt{internet} data set.}
  \label{fig:implmementations}
\end{figure}

\pagebreak

When looking at the complexities as shown in \autoref{tab:implementation-complexities}, we can see that the second implementation will only do better if the sizes of our graph $\log |V|$ and $\log |E|$ are significantly less important than the branching factor $B$. As the average branching factor is the number of edges divided by the number of nodes, this is unlikely, as these are usually of similar order of magnitude.

Additionally, the memory access pattern of trees is less friendly to memory caches than that of arrays, which also does not help in favour of the second implementation.
