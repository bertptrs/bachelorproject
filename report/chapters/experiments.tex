\chapter{Experiments}
\label{chp:experiments}

As a platform, we used the DAS 4 and its MPI implementation. We run algorithm with 8 instances per physical node, which is default for the DAS 4. % TODO standardize das notation
The DAS scheduler is responsible for placing algorithms on nodes. To circumvent jitter in timings results, we repeat each experiment 5 times. We vary the number of worker nodes in our experiment. Due to limitations of the platform, we can either run a multiple of 8 workers or less than eight workers. This is because DAS places 8 workers on a single physical platform.

\emph{Deze sectie mag weg/moet samen worden gevoegd met een soortgelijke sectie in Implementation}

\section{Experiment data}
For our experiment data, we used real-world graphs as well as generated graphs. For each graph we chose a specific source and sink for our experiments so that we could easily repeat and average results. An overview of the used graphs and configuration used can be found in \autoref{tab:data-overview}.

The University of Florida Sparse Matrix Collection \cite{FloridaSparseMatrix} for our real-world sample graphs. Even the collection is mostly unrelated to graphs, some matrices are in fact regular (natural) networks. We looked for well-connected graphs weighted graphs of a size for which max-flow computations take a significant but not too large time to complete. \autoref{tab:data-overview} shows t

To generate graphs, we looked to the GTgraph~\cite{bader2006gtgraph} toolkit. This program can generate graphs using any of four algorithms:
\begin{description}
    \item[random] allows us to specify a number of nodes $n$ and a number of edges $m$, after which it iteratively picks two random nodes and adds an edge between them.

    \item[Erd\H{o}s-Renyi] takes a number of edges $n$ and a probability $p$ of an edge between any pair of nodes existing. Then it iterates over all pairs of nodes and generates an edge with probability $p$. This is mostly equivalent to the \textbf{random} generator when $p = \frac{m}{n \times (n - 1)}$.

    \item[R-MAT] generates graphs that follow a power law for the degree distribution of the nodes. This means that for any degree $k$, $P(k) \propto k^{-\gamma}$.\footnote{In directed graphs nodes have two degrees, $k_{in}$ and $k_{out}$ because the number of edges coming in is not necessarily the same as the number of edges going out. The power law distribution then simply applies to both separately.} This is a property present in most networks. \emph{Hier moet nog een citaat bij, uit het boek voor Complex Networks.}
    
    \item[SSCA2] generates locally clustered graphs, meaning it creates graphs that are very well connected in small communities with little connections between them.
\end{description}

Because the both the random and Erd\H{o}s-Renyi algorithms do not produce graphs similar to those observed in real scenarios, we only consider graphs generated by the R-MAT and SSCA2 algorithms.

For each graph, we searched for a combination of source/sink that took a reasonable time to compute with one worker instance, in order to repeat the experiment and also had a non-zero final result so we could actually check the result. The resulting configurations can be found in \autoref{tab:data-overview}.

\begin{table}
	\centering

	\begin{tabularx}{\textwidth}{l | X | l | l | l | l | l}
		Name & Description & From & Nodes & Edges & Source & Sink \\
		\hline
		cage11 & DNA electrophoresis & \texttt{vanHeukelum/cage11} & 39082 & 559722 & 1361 & 28129 \\
		internet & Connectivity of internet routers & \texttt{Pajek/internet} & 124651 & 207214 & 94268 & 1046 \\
		rmat & Scale-free random graph & GTgraph R-MAT & 30000 & 5000000 & ? & ? \\
		ssca2 & Hierarchically clustered random graph & GTgraph SSCA2 & 32768 & 1570139 & 21264 & 7066 \\
	\end{tabularx}
	\caption{An overview of the data sets and configurations used.}
	\label{tab:data-overview}
\end{table}


\section{Expectations}
Graph algorithms generally do not parallellize well % TODO: Citation needed
so we do not expect major speedups. Any speedups will be visible the most when we use 8 or less instances. This is because the overhead of MPI communication within one physical node is less than between different nodes. As such, a performance drop is to be expected when a second node is added.

\section{Results}
After running every implementation of a varying amount of MPI workers, we can compare the execution time of each algorithm. For the results, we have looked at algorithm execution time, and have excluded the time needed for the MPI system to initialize.

This is because the overhead of setting up the parallel system is very much platform dependent, and not related to the problem we try to solve. The MPI initialization can be reused as well without overhead, so the cost is negligible for real world scenarios. Furthermore, as shown in \autoref{fig:initialization_time}, the cost of initializing does not increase that much with the number of workers.

\begin{table}
    \centering
    \input{generated/runtime-table.tex}
    
    \caption{Mean and standard deviation for algorithm run times in several configurations.}
\end{table}

\begin{figure}
	\includegraphics[width=\textwidth]{graphs/graph_initialization}
  \caption{Cost of initialization as a function of the number of workers.}
  \label{fig:initialization_time}
\end{figure}

\subsection{Relative speed up}
The first thing to notice is that there is very little speedup, if any, when the algorithm is run in parallel, as seen in \autoref{fig:speedup_cage11}.\footnote{Only the fastest implementation was considered for the speedup graph. Other implementations followed a similar pattern.} The timings are more or less the same until 8 workers, after which there is a slight slowdown. This can be explained by the increased latency when communicating between physical nodes.

The speed up graph does show a slight speedup when using two workers instead of one, but this is negligible. What is interesting, is the speed up is not declining or increasing, but actually very constant. This is an indication that the overhead from communication is being compensated by the parallelism. This means we can improve this figure by using a more efficient way of communicating.\footnote{In fact, the results produced by Hong et al \cite{LockFreeMultithreadedMaxFlow} on a shared memory implementation prove this, as shared memory has a lower overhead than a message passing implementation such as this one.}

\begin{figure}
  \includegraphics[width=\textwidth]{graphs/graph_speedup}
  \caption{Relative speedup when running a specific route on the datasets.}
  \label{fig:speedup_cage11}
\end{figure}

\subsection{Comparing implementations}

\begin{figure}
  \includegraphics[width=\textwidth]{graphs/graph_implementations}
  \caption{Comparison of implementations by execution time.}
  \label{fig:implmementations}
\end{figure}

Even if parallel implementations do not provide a good result, we can still look at how different implementations compare. \autoref{fig:implmementations} shows that our first implementation is without exception the better one.
