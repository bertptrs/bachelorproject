\chapter{Implementation}

\section{Push-lift}

The algorithm we consider the most suitable for optimization, is the Push Lift algorithm by Bo Hong et al. This is because it can be executed in parallel as it was designed that way and because it requires no locking between workers during execution. It uses the same preflow mechanics as the Push-Relabel \cite{ANewApproachToTheMaxFlowProblem} but has a few slight differences.

The algorithm considers a as a collection of nodes $V$ and a capacity function $c(u, v)$ indicating the remaining capacity from $u$ to $v$. Like the Push-Relabel algorithm, Push-Lift first establishes a preflow, in which some nodes have more incoming flow than outgoing flow. This is done by using each edge going out of a source to its full capacity.

The Push-Lift algorithm also has a similar height function $H$. At the start of the algorithm, the height function is initialized as follows:

$$
H(u) \gets \begin{cases}
	|V| & \text{if } u \text{ is a source} \\
	0 & \text{otherwise}
\end{cases}
$$

In order to construct a valid flow, the algorithm then looks at each active node  and attempts to \emph{push} some of the excess incoming flow to outgoing edges. A node $u$ must push to its lowest neighbour $v$ for which there is still capacity left in the connecting edge, and $H(u) > H(v)$. This is different from the Push-Relabel algorithm, where a push is only allowed from $u$ to $v$ when $H(u) = H(v) + 1$, with no further preference over which node to choose.

If no such push is possible, the node must be lifted. A \emph{lift} operation changes the height of node $u$, such that $H(u) \gets \min\{H(v) | c(u, v) > 0\} + 1$, that is, to the minimum of all neighbours of $u$ that have capacity in the edges plus $1$, so that in the next iteration, a push is possible.

Nodes are distributed over workers, which can work on either one node or a set of nodes. The algorithm terminates when there are no longer nodes with more incoming flow than outgoing flow, bar the sink nodes. This algorithm can find the maximum flow in $O(|V|^2|E|)$ operations \cite{LockFreeMultithreadedMaxFlow}.

\section{\textit{forelem} implementation}
\label{sec:forelem-pushlift}

To implement the above algorithm in forelem, we need to structure our data as sets of tuples. As the algorithm considers properties on edges, being the remaining capacity they have, and nodes, being their height and current excess flow.

It then makes sense to consider two sets of tuples. One of edges $E$ containing tuples $(u, v, c, f)$ with the edge source, sink, capacity, and current flow, and one of nodes $V$ containing tuples $(u, h, e)$, node, height, and excess. The last has one record per $u$, and is (for integer $u$) probably better off being an array or a map. In the following implementation, we will consider it as such.

We use a capacity and a remaining capacity for the flow, so that we can combine $c(u, v)$ and $c(v, u)$ in one tuple, instead of 2. This allows the algorithm to be simpler. Also, this circumvents the limitation that it is very difficult in \forelem to work on one tuple, and then to modify another.\footnote{It should be noted however, that this is not impossible. Instead, you could make a \forelem loop that you know will only match a single tuple. This does add a lot of loops, with a lot of variations that will not be efficient.}

The finalized implementation for the Push-Lift algorithm can be found in Appendix~\ref{app:pushlift}.

\section{DAS-4 and MPI}
\label{sec:das4}

We chose the DAS-4 as our experimentation platform. This means any parallelization would be done using MPI. This poses a slight challenge, as Hong's Push-Lift was previously engineered for and profiled on a shared memory system. MPI has no such thing, and instead relies on message passing for communication.

To work around this, we implement a solution in which each worker has knowledge of the updates that are relevant to that worker.

Firstly, we need to divide the work. This is done by modulo division, such that every worker has all nodes $u : u \mod	 workerCount = workerNo$. Then, we need to know what information is important which nodes.

For push operations, this is obvious, as both the endpoints of the edge being pushed over need to know what is happening. This means that we need pass at most one message, because at least one of the nodes is owned by the worker performing the operation. When a push operation is performed, a check is done to see if the destination has the same owner as the origin, and if not, a tuple $(u, v, \delta)$ is sent to communicate the push happening.

For lift operations, this is slightly more complicated. As any worker that could theoretically push to a given node needs to know that node's height, all information must be pushed there as well. To implement this, we precompute the set of neighbours for each node, and store those sets. When a lift operation is performed on $u$, all nodes $v : (u, v, c, f) \in E$ are sent the increment in height. This ensures that multiple consecutive lift operations do not interfere.

Finally, we need to ensure termination. In a shared memory implementation, one could regularly check the current excess at every node, and terminate if there are no more active nodes. Because we attempt to minimize communication, no single worker will necessarily know all excess flows, and more importantly, no single worker can be sure there are no messages in transit that will change this. To deal with this, we implement Safra's algorithm.

\subsection{Safra's algorithm}
\label{sec:safras-agorithm}

Safra's algorithm\cite{fokkink2013distributed} works by repeatedly passing along the workers a token. It works by passing a token along the workers. The token is filled with a color, black or white, and a message counter. If the message counter indicates there are no messages in transit, and the color indicates there are no more active workers.

This way, Safra's algorithm ensures that every node is inactive, and that there is no way a node could become active again. Its overhead is minimal, as the token is only passed when a node becomes inactive, so it does not interrupt execution when a worker is busy. After the entire algorithm is finished, the token is passed at most 2 times through every node.

\section{Generating implementations}
\label{sec:implementations}

Due to time constraints, it was not possible to create a program that could materialize the \forelem implementation of Push-Lift (see appendix~\ref{app:pushlift}) into a compilable or runnable algorithm. Instead, we chose to implement the algorithm by hand and then vary the way we handle the indices for the graph structure. By materializing for the origin node of edges, we arrive at a structure similar to adjacency lists. These are illustrated as follows:

\begin{enumerate}
	\item For the first implementation, we use nested varying length arrays. Height and excess are stored per-node in a second array as tuples.
	\item The second implementation we constructed uses a sparse array (implemented as \texttt{map}, backed by a red-black tree) as the data structure for the capacities.
	\item Our final implementation was a naive one, with all edge stored a an array of tuples, with the index set computed within the loop. This would serve as a baseline, but it did not complete any of the test runs within the allowed time limit of 120 minutes\footnote{On the DAS-4, the job time limit is 120 minutes, but exceptions may be made outside of office hours. For the experiment to be repeated, however, shorter execution times were preferred.} and will therefore be excluded from the rest of this thesis.
\end{enumerate}

This in itself provides us with some complexities for some operations associated with the algorithm. These complexities are shown in \autoref{tab:implementation-complexities}. Here, $|V|$ denotes the number of nodes, $|E|$ the number of edges, and $B$ the branching factor of the graph, which is the average number of edges per node.

From the complexities itself we can see that the first implementation has a trade-off compared to the second one. In the former we can find a particular edge in our data in linear time with respect to the branching factor, while for the latter we need a logarithmic time with respect to the total number of edges. In the next chapter we will see that the average branching factor in data sets is slightly larger than the binary logarithm of the number of nodes.  However, the cache access pattern for the first implementation is better (sequential scan as opposed to random access) so this difference may not be insignificant.

\begin{table}
\centering
\begin{tabular}{l||l|l|l}

	& Implementation 1 & Implementation 2 & Implementation 3 \\
\hline
Retrieve remaining capacity for edge & $O(B)$ & $O(\log |E|)$ & $O(|E|)$\\
Retrieve node excess / height & $O(1)$ & $O(\log |V|)$ & $O(1)$\\
Determine node neighbours & $O(B)$ & $O(B + \log |V|)$ & $O(|E|)$
\end{tabular}
\caption{Various complexities for the proposed implementations.}
\label{tab:implementation-complexities}
\end{table}
